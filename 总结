## Visualizing and Understanding Deep Neural Networks in CTR Prediction
1) 观察每一隐层的神经元状态的平均波动程度来判断是否过拟合。过拟合前，神经元波动程度较稳定，且测试和训练集较一致。
   过拟合后，波动程度加深，且训练集波动明显大于测试集。
2) 观察各特征组的梯度来判断特征影响力，梯度大的特征影响力也大。
3) 判断隐层是否必要，进而调整网络结构。 将隐层的点通过tsne投影到2维平面，然后看正负样本是否有明显的分离判断是否需要该层。
4) 使用probe方法，将隐层对样本的表征作为输入，训练逻辑回归模型，看逻辑回归模型的效果。
5) 对某一层神经元，评测训练数据和测试数据的均值、标准差、偏差以及峰度的区别，来判断训练数据和评测数据是否一致。
6) 了解边的权重，如果某个特征向上连接的权重太大，则说明这个特征可能过拟合了。
7) 可以通过扰动来探测特征的重要性，比如对稀疏ID的embedding，乘以一个均值为1而在0~2之间均匀分布的随机数

## Online Learning
在 在线学习中，顺序训练的方式比离线数据shuffle训练的方式更优。

## 基于深度学习的双11高维稀疏特征编码技术(排序)
1) 公域文章: https://mp.weixin.qq.com/s/Lf_UD92kgeTciqkHLyJmgg?spm=a2c4e.11153940.blogcont107807.5.411a4360xvbz0E
2) 里面提到将one-hot编码，变成six-hot编码，来对输入进行压缩，感觉six-hot编码的思想类似于bloom filter, 用多个hash function来编码
3) 各个域都存在冷门的特征，这些冷门的特征将会被热门的特征淹没，基本不起作用，跟全连接层的连接边权值会趋向于0，冷门的商品只会更冷门,
   这个的解决用挂靠编码来解决。热门商品采用one-hot表示，冷门商品采用M-hot表示，冷门商品如果通过i2i和热门商品建立连接，则冷门商品的最后一个hot和
   热门商品共享。
4) Google的模型是把embedding向量和统计特征放到同一个DNN网络中学习，但实验发现这样会削弱统计特征的作用。我们为统计特征专门又组建了一个包含2个隐层
   的网路，并且为了增强非线性效果，激活函数从RELU改为TanH/Sigmiod。

## 基于笔画embedding词向量
1) 为了提升词向量的表示，清华提出了CWE改进词向量的方法，中间target词向量还是用单个词表示，context词借用了单个汉字的embedding，
   c = 1/2 * (c0 + (1/N)*sum(h_k))，h_k是每个汉字对应的embedding
2) 本来还有引入偏旁的做法，但偏旁只是字的一部分信息，也有引入字的构件的做法，但构件和语义有分歧，比如"智"这个字，构件是矢，口，日，都和字义没有关系。
3) 为了进一步提高效果，提出了基于笔画的embedding的改进方法。中间target词向量还是用单个词表示，对于context词，将单个汉字的所有笔画列出来，
   然后收集所有的3元、4元、5元，将context词组织成n_gram,用sum(n_gram_embedding)来表示词
   
## DCN模型详解

