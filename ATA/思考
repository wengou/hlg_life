1. 125128
  1) 智能图文生成、布局
  2) 关于智能图文布局，排版结构有3种单栏、双栏、三栏，每一种上又可以加上推荐理由、优惠价格等多种装饰元素，
      这样叉乘下来有24种不同的布局，利用布局特征，用户特征，交叉特征学习不同用户群对布局的偏好。
  3) 在66892和125182这两篇文章中都提到如何进行流量分配的思路，66892这篇文章cij表示用户i对行业j的点击率，xij表示将行业j分配给用户i的概率，
     因此也是解带限制的优化问题，max Sum(xij*cij), st sum_j(xij) <= 1, sum_i(xij) = bi。 具体落地的框架是，基于上面的拉格朗日计算出一个调控因子，
     然后参照调控因子进行调控。
  4) 125182这篇文章中，有三种调权方式，[1] boost加权，对该类别下的所有商品都做了相同的累加 加权，缺点是会把一些差的商品也带上来；
     [2] weighted加权，乘上一个因子加权， 这个对商品有一定的区分度，原来ctr高的商品获得的影响也要大一些，这个效果比1好 [3] logitboost，
     根据ctr反推，logit乘以一个权重，再计算初ctr.
     
2. 125652
1) 第一代Query改写方法为SimRank++，其缺陷为主要考虑query和bidword之间的语义相关性，而未考虑bidword侧的流量机制，难以释放营收潜力。
2) 第二代参照雅虎的论文，Optimizing Query Rewrites For Keyword-based Advertising, 直接计算Query对应的优质广告集合，再反过来选取bidword，
实现了对广告的最大覆盖。该算法存在的缺陷，将整个Query视作一个整体，同时其原理是基于统计的记忆模型
3）第三代是DeepNB，对query进行分词处理，获取词对应的Embedding，将该Embedding一方面取平均，另一方面交给cnn捕获n-gram信息，cnn后接Attention网络，
   对于。 同时将头部query的ID直接Embedding，这是为了记住头部query的特性。将用户的信息也放进去，实现个性化的query改写。
   基于上面三个向量加权和再接三层FC。 bidword就直接根据ID embedding，然后计算DSSM。
4) 训练样本如何得到，如果将query点击过的广告的bidword都保留，样本就太多了，因此price(ad, bidword) / log(bidword的广告数 + 1)，
    用该分数对该广告下所有bid算分，进而计算百分比。 

3. 125424
1) 关于Attention GRU的改进思路可以借鉴：[1] 不同的行为有不同的权重，加购的权重比点击的权重要更大，因此对原lstm的d维的hidden state
   重新做了一个投影(用一个d*d维的矩阵) [2] 引入时间衰减因子，不同时间的行为对当前的影响也不同。[3] 排序的时候要考虑trigger item的信息。
   
4. 121438
1) 基于异构网络元路径embedding表征的大规模关键词推荐
2) 所做的事情就是在手淘的搜索框中，推荐默认的搜索词，做法是采用基于异构网络元路劲法
3) 具体做法：构建user-item-query的异构网络， 为每种类型的节点构造多条元路径，比如对user可以构造metapath1: user->item->query(用户点击过的商品，
    商品主要被哪些query引导)，也可以构建metapath2: user->query->item(用户的搜索query，query下点击的最多的item)。 
    对query和item的最开始的表示是基于分词后的term的embedding直接平均，然后对metapath1中的item，根据query来获得，user根据item来获得。
    对metapath2中的query根据item来获得，user跟军query来获得。 所以最原始的参数就是term对应的embbeding参数，这样做可以减少参数量。
    
5. 102403
1) 基于树的向量召回不能用概率连乘树，原因是word2vec中的概率连乘树适用于路径已经给定的情况下来计算概率，而用作召回时需要计算所有叶子节点的概率。
2) 另一个原因是，层次softmax倾向于将某种程度上相似的节点组织成兄弟节点，进行优和次优的判断。但推荐和召回往往是对相似的一类要么整体感兴趣，
   要么整体不感兴趣，因此建树的时候，兄弟节点间应该是不同的类。
3) 训练的时候，将用户当天行为的叶子节点(购买,点击)以及其所有祖先节点作为正例，其它同层的兄弟节点左右负样本。
4) 里面提到关于树构建的思路比较有借鉴意义，先基于原始的商品树进行训练，得到每个节点的向量，然后再对向量使用kmeans重新聚类。 

6. 127256
1) 迁移学习的有效性很大程度上取决于source domain和target domain之间的差异，如果差异很大，则迁移很可能是无效的。
2) 避免domain gap的方法通常使用样本迁移，找出合适的源样本去帮助target domain。该论文使用强化学习来提升样本迁移能力，找出合适的样本来帮助target domain
3) 论文Learning to selectively Transfer: Reinforced Transfer Learning for Deep Text Matching
4) 关于强化学习部分，对于source domain里的每个样本，RL会做一个action(选或者不选)，每个batch都有一个immediate reward r。对每个样本，其state由三个
部分组成，TL模型学到的表示h，源模型loss, target模型loss，源模型的prediction，target模型的prediction
5) Ruder and Plank提出了基于贝叶斯方法的样本选择算法，Learning to Select Data for Transfer Learning With Bayesian Optimization

7. 126584
1) 进行文本聚类时，使用了Kmedoids算法，另一种是使用了自下而上的层次聚类。
2) 图像相似度可以根据直方图，SSIM，MSE，SIFT， CNN来提取用于计算相似度的特征向量。
3) 文本聚类依赖于文本相似度计算，本文将文本相似度计算作为一个单独的环节拆分了出来。对于短文本，使用文本编辑距离计算相似度比较合适，对于长文本使用
SimHash + 简单共有词相似度即可。

8. 129556
1) AutoInt: XDeepFM存在耗时过大的问题;
2) 借助Transform网络结构的Encoder来融合不同field的embedding。
3) 该资料中有一些调参经验，[1]是在标准的google的网络中是MultiHead的Encoder是6层，这里尝试后3层的效果最佳，猜测和参与训练的样本量有关系;
   [2]该模型实验了控制参与特征交叉的特征field的个数，发现当特征数超过40时auc波动较大; [3] 模型实验了引入dropout，发现引入dropout后效果下降，
   分析原因可能是用户的行为本来就比较稀少，再引入dropout会导致用户信息不足。
   
 9. 130377
 1) 进行销量预测时，输入过去三个月的每天的订单数，成交数等数据，每天一个行向量(里面有相应的归一化以后的计数)，使用卷积神经网络来提取特征。
 2) 卷积时行向量方便将所有特征都输入，纵轴方向有一个filter size，原因是x轴方向没有局部性。
 3) 关于样本选择，可以去除那些label值过大的样本，因为这批样本会产生很大的误差，从而导致非常大的梯度，进而导致神经元挂掉。
 4) 对于回归问题，不适合使用dropout？这里的这个说法有点存疑。使用了L2正则，L2正则时对w进行正则，但不对偏执项进行正则。
 5) 销量预测属于长尾分布，偏差很大，因此不直接回归目标值，而是回归目标的log值或者目标的开根号值。
 6) 也引入了BN。
 
 10. 129130
 1) 对句子使用双向LSTM的方式来Encoding，然后再使用Attention来做融合，这里的一个变种是不是只使用单个attention来融合，而是使用了多个attention
 2) 使用多个Attention的目的是，不同的语义空间，不同的视角来看待问题。如果都简单的只在单一维度看问题，会导致类似语义中和的现象。
    比如"宝马"和"战车"，以及"宝马"和"奔驰"其实是不同的语义。
 3) 为了强迫从不同的视角看问题，强迫Attention的权重系数必须不一样，因此引入了|| A*A` - I ||这个机制范数。
 
 11. 55560
 1) 关于Query改写，原来传统的路子是f(P(bi|q)*f(ai|bi, q))~f(P(bi|q)*f(ai|bi))。
 2) 但其实用户在query是否点击某个广告和bid没有任何关系，query->bid->ad的三级跳只是商业模式而已。
 3) 因此一个思路是，先找出query下那些效果好的广告，然后再找出对应的bid。
 4) Yahoo有一篇类似的思路是，Optimizing Query Rewrite for Keyword-Based Advertising
 5) 有一篇衡量语义相关性的资料Simrank++: Query Rewriting through link analysis of the click graph
 
 12. 125262
 1) NodeBidding存在如下的问题，[1]将query看做一个不可分割的graph node，比如大码女装和胖mm衣服从语义上来说是接近的，但NodeBidding将其
 视作两个不同的词。[2] NodeBidding本质上基于统计的，因此在长尾流量上挖掘出高置信度的query改写比较困难。
 2) DeepNB模型使用了词的信息，对左侧的query，使用了词的embedding，基于词的embedding一个是average pooling，另一个是先做卷积，卷积可以捕获
    n-gram的信息，卷积以后引入了Attention based pooling。
 3) 关于样本的获取，没有query和bid word的直接label，通过query下点击的ads对应的bid来获取正样本，为了减少样本量，对广告所有bid word采样，正比于该
 4) bid的price，反比于log(bid_ads_count + 1)，负样本是从由易到难各级别query都有，包括不同根目录下广告购买的bidword,
    不同叶子类目下广告购买的bidword, 同一广告下q值偏小的bidword，同一广告下q值偏大的bidword。
    
  13. 127743
  1) 关于Mutlitask-Learning，如果多个task之间存在较大的差异，论文实现了一种Go-MTL，定义k个basic task，使用这k个basic task来做线性组合
  2) 
  
  14. 127927
  1） 召回优化:[1] 使用RandomWalk, Word2Vec, Similarity来增加i2i召回的比例，减少热门打底的数量; [2] 进一步引入了DeepMatch来扩充i2i的数量；
      [3] 80%的top2,top3 item是根据shop2i召回的样本，这部分样本的ctr不高，因此开发了Shop-inside的i2i，估计是shop2item + swing i2i搞定
      
  15. 125424
  1) 关于交互式推荐，KDD2018的资料 A Two-Stage Approach toward Interactive Recommendation
  2) 所谓交互式，就是根据用户浏览的item生成一个问题，根据用户的反馈获取用户的即时兴趣，进而进行后续的推荐。
  3) 在本文中，交互推荐的问题产生是以推荐关键词的方式来完成的，关键词的候选是来自搜索词。
  4) 
 
