1. 125128
  1) 智能图文生成、布局
  2) 关于智能图文布局，排版结构有3种单栏、双栏、三栏，每一种上又可以加上推荐理由、优惠价格等多种装饰元素，
      这样叉乘下来有24种不同的布局，利用布局特征，用户特征，交叉特征学习不同用户群对布局的偏好。
  3) 在66892和125182这两篇文章中都提到如何进行流量分配的思路，66892这篇文章cij表示用户i对行业j的点击率，xij表示将行业j分配给用户i的概率，
     因此也是解带限制的优化问题，max Sum(xij*cij), st sum_j(xij) <= 1, sum_i(xij) = bi。 具体落地的框架是，基于上面的拉格朗日计算出一个调控因子，
     然后参照调控因子进行调控。
  4) 125182这篇文章中，有三种调权方式，[1] boost加权，对该类别下的所有商品都做了相同的累加 加权，缺点是会把一些差的商品也带上来；
     [2] weighted加权，乘上一个因子加权， 这个对商品有一定的区分度，原来ctr高的商品获得的影响也要大一些，这个效果比1好 [3] logitboost，
     根据ctr反推，logit乘以一个权重，再计算初ctr.
     
2. 125652
1) 第一代Query改写方法为SimRank++，其缺陷为主要考虑query和bidword之间的语义相关性，而未考虑bidword侧的流量机制，难以释放营收潜力。
2) 第二代参照雅虎的论文，Optimizing Query Rewrites For Keyword-based Advertising, 直接计算Query对应的优质广告集合，再反过来选取bidword，
实现了对广告的最大覆盖。该算法存在的缺陷，将整个Query视作一个整体，同时其原理是基于统计的记忆模型
3）第三代是DeepNB，对query进行分词处理，获取词对应的Embedding，将该Embedding一方面取平均，另一方面交给cnn捕获n-gram信息，cnn后接Attention网络，
   对于。 同时将头部query的ID直接Embedding，这是为了记住头部query的特性。将用户的信息也放进去，实现个性化的query改写。
   基于上面三个向量加权和再接三层FC。 bidword就直接根据ID embedding，然后计算DSSM。
4) 训练样本如何得到，如果将query点击过的广告的bidword都保留，样本就太多了，因此price(ad, bidword) / log(bidword的广告数 + 1)，
    用该分数对该广告下所有bid算分，进而计算百分比。 

3. 125424
1) 关于Attention GRU的改进思路可以借鉴：[1] 不同的行为有不同的权重，加购的权重比点击的权重要更大，因此对原lstm的d维的hidden state
   重新做了一个投影(用一个d*d维的矩阵) [2] 引入时间衰减因子，不同时间的行为对当前的影响也不同。[3] 排序的时候要考虑trigger item的信息。
   
4. 121438
1) 基于异构网络元路径embedding表征的大规模关键词推荐
2) 所做的事情就是在手淘的搜索框中，推荐默认的搜索词，做法是采用基于异构网络元路劲法
3) 具体做法：构建user-item-query的异构网络， 为每种类型的节点构造多条元路径，比如对user可以构造metapath1: user->item->query(用户点击过的商品，
    商品主要被哪些query引导)，也可以构建metapath2: user->query->item(用户的搜索query，query下点击的最多的item)。 
    对query和item的最开始的表示是基于分词后的term的embedding直接平均，然后对metapath1中的item，根据query来获得，user根据item来获得。
    对metapath2中的query根据item来获得，user跟军query来获得。 所以最原始的参数就是term对应的embbeding参数，这样做可以减少参数量。
    
5. 102403
1) 基于树的向量召回不能用概率连乘树，原因是word2vec中的概率连乘树适用于路径已经给定的情况下来计算概率，而用作召回时需要计算所有叶子节点的概率。
2) 另一个原因是，层次softmax倾向于将某种程度上相似的节点组织成兄弟节点，进行优和次优的判断。但推荐和召回往往是对相似的一类要么整体感兴趣，
   要么整体不感兴趣，因此建树的时候，兄弟节点间应该是不同的类。
3) 训练的时候，将用户当天行为的叶子节点(购买,点击)以及其所有祖先节点作为正例，其它同层的兄弟节点左右负样本。
4) 里面提到关于树构建的思路比较有借鉴意义，先基于原始的商品树进行训练，得到每个节点的向量，然后再对向量使用kmeans重新聚类。 

6. 127256
1) 迁移学习的有效性很大程度上取决于source domain和target domain之间的差异，如果差异很大，则迁移很可能是无效的。
2) 避免domain gap的方法通常使用样本迁移，找出合适的源样本去帮助target domain。该论文使用强化学习来提升样本迁移能力，找出合适的样本来帮助target domain
3) 论文Learning to selectively Transfer: Reinforced Transfer Learning for Deep Text Matching
4) 关于强化学习部分，对于source domain里的每个样本，RL会做一个action(选或者不选)，每个batch都有一个immediate reward r。对每个样本，其state由三个
部分组成，TL模型学到的表示h，源模型loss, target模型loss，源模型的prediction，target模型的prediction
5) Ruder and Plank提出了基于贝叶斯方法的样本选择算法，Learning to Select Data for Transfer Learning With Bayesian Optimization

7. 126584
1) 进行文本聚类时，使用了Kmedoids算法，另一种是使用了自下而上的层次聚类。
2) 图像相似度可以根据直方图，SSIM，MSE，SIFT， CNN来提取用于计算相似度的特征向量。
3) 文本聚类依赖于文本相似度计算，本文将文本相似度计算作为一个单独的环节拆分了出来。对于短文本，使用文本编辑距离计算相似度比较合适，对于长文本使用
SimHash + 简单共有词相似度即可。

8. 129556
1) AutoInt: XDeepFM存在耗时过大的问题;
2) 借助Transform网络结构的Encoder来融合不同field的embedding。
3) 该资料中有一些调参经验，[1]是在标准的google的网络中是MultiHead的Encoder是6层，这里尝试后3层的效果最佳，猜测和参与训练的样本量有关系;
   [2]该模型实验了控制参与特征交叉的特征field的个数，发现当特征数超过40时auc波动较大; [3] 模型实验了引入dropout，发现引入dropout后效果下降，
   分析原因可能是用户的行为本来就比较稀少，再引入dropout会导致用户信息不足。
   
 9. 130377
 1) 进行销量预测时，输入过去三个月的每天的订单数，成交数等数据，每天一个行向量(里面有相应的归一化以后的计数)，使用卷积神经网络来提取特征。
 2) 卷积时行向量方便将所有特征都输入，纵轴方向有一个filter size，原因是x轴方向没有局部性。
 3) 关于样本选择，可以去除那些label值过大的样本，因为这批样本会产生很大的误差，从而导致非常大的梯度，进而导致神经元挂掉。
 4) 对于回归问题，不适合使用dropout？这里的这个说法有点存疑。使用了L2正则，L2正则时对w进行正则，但不对偏执项进行正则。
 5) 销量预测属于长尾分布，偏差很大，因此不直接回归目标值，而是回归目标的log值或者目标的开根号值。
 6) 也引入了BN。
 
 10. 129130
 1) 对句子使用双向LSTM的方式来Encoding，然后再使用Attention来做融合，这里的一个变种是不是只使用单个attention来融合，而是使用了多个attention
 2) 使用多个Attention的目的是，不同的语义空间，不同的视角来看待问题。如果都简单的只在单一维度看问题，会导致类似语义中和的现象。
    比如"宝马"和"战车"，以及"宝马"和"奔驰"其实是不同的语义。
 3) 为了强迫从不同的视角看问题，强迫Attention的权重系数必须不一样，因此引入了|| A*A` - I ||这个机制范数。
 
 11. 55560
 1) 关于Query改写，原来传统的路子是f(P(bi|q)*f(ai|bi, q))~f(P(bi|q)*f(ai|bi))。
 2) 但其实用户在query是否点击某个广告和bid没有任何关系，query->bid->ad的三级跳只是商业模式而已。
 3) 因此一个思路是，先找出query下那些效果好的广告，然后再找出对应的bid。
 4) Yahoo有一篇类似的思路是，Optimizing Query Rewrite for Keyword-Based Advertising
 5) 有一篇衡量语义相关性的资料Simrank++: Query Rewriting through link analysis of the click graph
 
 12. 125262
 1) NodeBidding存在如下的问题，[1]将query看做一个不可分割的graph node，比如大码女装和胖mm衣服从语义上来说是接近的，但NodeBidding将其
 视作两个不同的词。[2] NodeBidding本质上基于统计的，因此在长尾流量上挖掘出高置信度的query改写比较困难。
 2) DeepNB模型使用了词的信息，对左侧的query，使用了词的embedding，基于词的embedding一个是average pooling，另一个是先做卷积，卷积可以捕获
    n-gram的信息，卷积以后引入了Attention based pooling。
 3) 关于样本的获取，没有query和bid word的直接label，通过query下点击的ads对应的bid来获取正样本，为了减少样本量，对广告所有bid word采样，正比于该
 4) bid的price，反比于log(bid_ads_count + 1)，负样本是从由易到难各级别query都有，包括不同根目录下广告购买的bidword,
    不同叶子类目下广告购买的bidword, 同一广告下q值偏小的bidword，同一广告下q值偏大的bidword。
    
  13. 127743
  1) 关于Mutlitask-Learning，如果多个task之间存在较大的差异，论文实现了一种Go-MTL，定义k个basic task，使用这k个basic task来做线性组合
  2) 
  
  14. 127927
  1） 召回优化:[1] 使用RandomWalk, Word2Vec, Similarity来增加i2i召回的比例，减少热门打底的数量; [2] 进一步引入了DeepMatch来扩充i2i的数量；
      [3] 80%的top2,top3 item是根据shop2i召回的样本，这部分样本的ctr不高，因此开发了Shop-inside的i2i，估计是shop2item + swing i2i搞定
      
  15. 125424
  1) 关于交互式推荐，KDD2018的资料 A Two-Stage Approach toward Interactive Recommendation
  2) 所谓交互式，就是根据用户浏览的item生成一个问题，根据用户的反馈获取用户的即时兴趣，进而进行后续的推荐。
  3) 在本文中，交互推荐的问题产生是以推荐关键词的方式来完成的，关键词的候选是来自搜索词。
  4) 用户点击了某个关键词，则认为用户对该关键词感兴趣，否则就是对该关键词不感兴趣。将点击的关键词作为特征输入到模型中，就是标准的搜索流程了。
  5) 关键词推荐的召回，通过u2i2q, u2i2scene2q, u2i2c2q, 作者是把user, query, item, scene, category都作为异构网络的节点放到异构网络中，通过
  异构网络来完成召回的。Meta-graph based recommendation fusion over heterogeneous information networks.
  6) 在排序模型中，使用了一个Attention-GRU的思路，除了Attention-GRU的思路，不同类型的行为的影响不一样，为每种类型的行为弄了一个d*d维的M矩阵，
  将原来rnn里的hidden state乘以投影矩阵。另外，hidden state还要再乘以一个时间衰减因子。
  
  16. 124362
  1) 对于单个item总共有id, title, image, statistic几个不同维度的特征，对每个特征都会提取出模态不变性特征和模态独特性特征。我理解为什么要
  对特征抽取不变性特征和独特性特征，便于后续对模态独特性特征进行Attention。多种模态特征可能包含不利于区分不同模态的贡献的冗余信息。
  在计算模态的不同贡献时，应该消除冗余特征。
  2) 论文对每种特征都提取出模态独特的embedding和模态共享的embedding。共享的embedding是对所有模态使用相同的投影矩阵来获得。
     独特embedding会用attention将不同模态的独特embedding融合起来。计算attention分数的时候，除了输入模态的embedding，还需要
     使用category的embedding。
  3) 用户历史行为使用gru串联起来，gru的更新门里把行为属性的embedding也考虑进来了，包括(距离现在的时间，以及行为类型的embedding)。
独特

17. 121438
1) 基于异构网络, 使用metapath来做召回;
2) query, item, user是三种不同类型的节点，首先将这三种不同类型的节点全部用term的embedding表示，这样，这三种类型的节点就在相同的空间里。
3) 使用类似于标签传播的方法，从metapath元路径的最后一层节点逐层往回传播。所谓metapath，就是一条定义好了各个节点类型的路径，比如user->item->query。
4) 定义多条元路径，每条元路径都可以求得一个user embedding，取平均后就可以获得user的embedding。
5) 获得的user metapath embedding和query metapath embedding当做特征输入到全连接网络里去。

18. 73892
1) 广告主指定宝贝ID，自动进行素材提取工作，尺寸适配，创意在线拼接工作;
2) 模板库中有多种模板，自动实现素材和模板的匹配;

19. 124035
1) 如何实现对新品的提速，主要就在于添加了User2New(User to New Item的 引导、收藏、加购、点击率、转化率等)，以及Query2New(Query to New Item)的引导、
收藏、加购、点击率、转化率等。该特征除了加在Deep侧，也加在Wide侧和ItemId做交叉，实现快速记忆Item对哪些Query更敏感。

20. 125214
1) 该文基于用户过去的点击序列进行召回。
2) 点击序列使用双层LSTM来对过去点击过的item进行Embedding，一个技巧是双层LSTM之间添加dropout + 残差网络来防止过拟合;
3) 点击序列经过LSTM Embedding后，使用Masked Multi-head Self-Attention进行处理，Attention是参照Attention is All you need的做法，在点击序列间
进行Attention。Multi-head的结果concat起来，然后在用一个投影矩阵投影到低维空间。
4) User基本信息怎样和点击序列融合在一起呢，这里也采用了Attention的机制，先将User的基本信息用全连接网络进行投影，投影到和点击序列相同的空间，然后
再和点击序列的Embedding做Attention。
5) 关于负样本的处理，根据商品的频次做负采样处理；关于正样本的处理，将后续几个点击样本都作为正样本， 变成Multi-Label的方式。
6) 点击序列只是代表了用户的短期兴趣，要将用户的长期类目偏好，品牌品号，将这些兴趣Embedding后和UserEmbedding做Attention，得到用户的长期偏好，
长期偏好和短期
6) 点击序列只是代表了用户的短期兴趣，要将用户的长期类目偏好，品牌品号，将这些兴趣Embedding后和UserEmbedding做Attention，得到用户的长期偏好，长期偏好和
