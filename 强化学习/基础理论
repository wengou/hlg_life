 第二章 马尔科夫决策过程
 
 1. 包含如下几个要素
 1) S 为有限的状态集
 2) A 为有限的动作集
 3) P 为状态转移概率矩阵, P表示在状态s下，采用动作a, 转移到其它各个状态s'的概率
 4) R 为回报函数
 5) r 为折扣因子，用来计算累积回报
 
 2. 强化学习的目标 
 1)  给定马尔科夫决策过程和策略π，可以计算每个状态s的累积回报的期望，即状态值函数;
 2)  给定马尔科夫决策过程和策略π，可以计算每个状态-动作(s,a)下累积回报的期望，即状态-行为值函数;
 3)  强化学习的目标是找到最优策略π，使得累计回报的期望最大;
 
 
 第三章 基于模型的动态规划
 1) 根据转移概率P是否已知，可以分为基于模型的动态规划方法和基于无模型的强化学习方法。
 2) 这两种类型都包含策略迭代算法，值迭代算法和策略搜索算法。
 3) 如果转移概率P已知，则每个状态的值函数V(s)可以表示成 基于策略π、转移概率P、回报函数R、折扣因子r，以及后续状态s'值函数的等式，
    然后就可以用高斯-赛德尔迭代算法(雅克比)求解，这就是策略评估算法;
 4) 
 

