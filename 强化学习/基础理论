 第二章 马尔科夫决策过程
 
 1. 包含如下几个要素
 1) S 为有限的状态集
 2) A 为有限的动作集
 3) P 为状态转移概率矩阵, P表示在状态s下，采用动作a, 转移到其它各个状态s'的概率
 4) R 为回报函数
 5) r 为折扣因子，用来计算累积回报
 
 2. 强化学习的目标 
 1)  给定马尔科夫决策过程和策略π，可以计算每个状态s的累积回报的期望，即状态值函数;
 2)  给定马尔科夫决策过程和策略π，可以计算每个状态-动作(s,a)下累积回报的期望，即状态-行为值函数;
 3)  强化学习的目标是找到最优策略π，使得累计回报的期望最大;
 
 
 第三章 基于模型的动态规划
 1) 根据转移概率P是否已知，可以分为基于模型的动态规划方法和基于无模型的强化学习方法。
 2) 这两种类型都包含策略迭代算法，值迭代算法和策略搜索算法。
 3) 如果转移概率P已知，则每个状态的值函数V(s)可以表示成 基于策略π、转移概率P、回报函数R、折扣因子r，以及后续状态s'值函数的等式，
    然后就可以用高斯-赛德尔迭代算法(雅克比)求解，反复迭代直到达到V(s)的稳定值，这就是策略评估算法;
 4) 策略评估获得状态值函数V(s) => 计算状态-行为值函数Q(s, a) => 为每个状态s选择最大Q(s, a)最大的行为a => 进行策略改进 
    => 重复前面几步直到策略稳定 => 完成策略改进，称为策略迭代法
 5) 不一定要等到策略评估算法完全收敛，可以在迭代一次(更新一次V(s))后，立刻进行策略改善，直到策略稳定，称之为值函数迭代法;
    具体做的时候，不是 更新V(s) => 改进策略 => 更新V(s)，直到策略π稳定，而是将两步合成一步，V(s) = max(R_a + r*sum_s'(P*V(s'))),
    不断循环该过程，直到V(s)稳定，再根据V(s)选出最优的策略π
 
 第四章 无模型的基于蒙特卡罗的强化学习方法
 1) 当不知道转移概率时，只能通过统计平均值的方法求得(s, a)对的Q(s, a)的值
 2) 第一步是初始化，将Q(s, a)设置为随机值，策略π设置为随机值
 3) 进行探索的时候，为保证每个(s, a)都没探索到，一般会采用ε-soft的策略
 4) 此时的过程为： [1] 从某个(s0,a0)使用ε-soft策略前进直到终止状态，得到一次实验episode;[2]对该episode中每个出现的(s,a)
    计算其第一次出现时的累积回报G，进而更新其Q(s, a) [3] 

